import numpy as np
import pandas as pd

"""
Pandas的主业是数据分析。因此，从外部文件读/写数据，属于Pandas的重要组成部分。
Pandas提供了多种API函数，以支持多种类型数据（如CSV、Excel、SQL等）的读写.
"""
# TODO 1.利用Pandas读取文件
"""
Pandas可以将读取到的表格型数据，转换成DataFrame类型的数据，
然后通过操作DataFrame进行数据分析、数据预处理及行和列的操作等。
Pandas的核心在于数据分析，而不是数据文件的读取和写入。
下面我们以CSV文件的读写为例，来讨论一下Pandas是如何处理文件的，其他类型文件的操作也是类似的。

为了适应各种应用场景，read_csv()方法还配置了大量可用的参数，这里我们仅说明部分常用的参数。


"""
df1 = pd.read_csv(filepath_or_buffer="./mygo_danmaku/ep01.csv",sep=',')
# print(df1)

"""
      PROGRESS                CTIME  ...  FONTSIZE            TEXT
0        0.000  2023-08-11 19:43:01  ...        25          梦结束的地方
1        0.000  2023-08-11 20:17:49  ...        25          梦开始的地方
...        ...                  ...  ...       ...             ...
8603  1421.137  2023-09-25 11:39:59  ...        25          爱音的大长腿
8604  1425.900  2023-09-26 19:27:05  ...        25         双排被坑后的你

[8605 rows x 8 columns]
"""

"""
read_csv(filepath_or_buffer,sep=',',delimiter=None,header='infer',
names=None,index_col=None,converters=None,data_parser=None,...,

filepath_or_buffer：指定要读取的数据源，可以是网络链接地址URL，也可以是本地文件。
sep：指定分隔符（Separator），如果不指定参数，默认将英文逗号作为数据字段间的分隔符号。
delimiter：定界符，备选分隔符（如果指定该参数，则前面的sep参数失效），支持使用正则表达式来匹配某些不标准的CSV文件。Delimiter可视为sep的别名。
header：指定行数作为列名（相当于表格的表头，用来说明每个列的字段含义），如果文件中没有列名，则默认为0（即设置首行作为列名，真正的数据在0行之后）。如果没有表头，则起始数据就是正式的待分析数据，此时这个参数应该设置为None。
index_col：指定某个列（比如ID、日期等）作为行索引。如果这个参数被设置为包含多个列的列表，则表示设定多个行索引。如果不设置，Pandas会启用一个0～n-1（n为数据行数）范围内的数字作为列索引。
converters：用一个字典数据类型指明将某些列转换为指定数据类型。在字典中，key用于指定特定的列，value用于指定特定的数据类型。
"""

# TODO 2.DataFrame中的常用属性
"""
一旦我们把数据正确读取到内存之中，形成一个DataFrame对象，就可以“循规蹈矩”地使用各种属性或方法来访问、修改DataFrame对象中的数据。

首先，在Pandas中，所谓的“object”类型在本质上就是Python中的字符串类型，其主要用途就是存储文本类型的数据；
另外，如果我们想查看多列（≥2）数据类型，用的属性是dtypes，而查询单列数据类型时用的属性是dtype.
"""
# print(df1.dtypes)  # 通过dtypes属性来查看DataFrame中各个列的数据类型。
# print(df1['ID'].dtype)
# print(df1[['ID','PROGRESS']].dtypes)
"""
PROGRESS    float64
CTIME        object
ID            int64
HASH         object
COLOR        object
MODE          int64
FONTSIZE      int64
TEXT         object
dtype: object

int64

ID            int64
PROGRESS    float64
dtype: object
"""

# print(df1.columns)  # 查看DataFrame各个列的名称
# print(df1.axes)     # 行标签和列标签
# print(df1.ndim)     # 维度数                    2
# print(df1.shape)    # 维度信息                  (8605, 8)
# print(df1.size)     # 元素个数                  68840
# print(df1.values)   # 数值，类似于无行标签和列标签的Numpy数组

"""
Index(['PROGRESS', 'CTIME', 'ID', 'HASH', 'COLOR', 'MODE', 'FONTSIZE', 'TEXT'], dtype='object')

[RangeIndex(start=0, stop=8605, step=1), Index(['PROGRESS', 'CTIME', 'ID', 'HASH', 'COLOR', 'MODE', 'FONTSIZE', 'TEXT'], 
dtype='object')]

[[0.0 '2023-08-11 19:43:01' 1413322056778563840 ... 5 25 '梦结束的地方']
 [0.0 '2023-08-11 20:17:49' 1413322111279350016 ... 5 25 '梦开始的地方']
 ...
 [1421.137 '2023-09-25 11:39:59' 1413664989121959168 ... 1 25 '爱音的大长腿']
 [1425.9 '2023-09-26 19:27:05' 1414624864693758208 ... 5 25 '双排被坑后的你']]
"""

# TODO 3.DataFrame中的常用方法
"""
head([n])/tail([n])、describe()、max()/min()、mean()/median()、
std()、sample([n])、dropna()、count()、value_counts()、groupby()
"""

# head,tail
"""
head()和tail()方法中其实是有参数的，其参数默认值为5。所以，如果我们不设置数值就会默认显示前5行。
如果我们想显示前10行，则需要显式指定这个参数值。
"""
# print(df1.head())
# print(df1.head(10))
# print(df1.tail())
# print(df1.tail(10))
"""
   PROGRESS                CTIME  ...  FONTSIZE        TEXT
0       0.0  2023-08-11 19:43:01  ...        25      梦结束的地方
1       0.0  2023-08-11 20:17:49  ...        25      梦开始的地方
2       0.0  2023-08-12 17:24:16  ...        25     我 邦 要 火
3       0.0  2023-08-19 18:44:18  ...        25  不要下架啊，求求你了
4       0.0  2023-08-20 09:37:18  ...        25    组一辈子乐队！​

[5 rows x 8 columns]

      PROGRESS                CTIME  ...  FONTSIZE            TEXT
8600  1417.573  2023-09-25 13:46:32  ...        25            漫漫长途
8601  1418.006  2023-08-12 11:13:39  ...        25          她要吃人了！
8602  1418.552  2023-08-11 19:55:28  ...        25  现在看这个微笑，后背有点发凉
8603  1421.137  2023-09-25 11:39:59  ...        25          爱音的大长腿
8604  1425.900  2023-09-26 19:27:05  ...        25         双排被坑后的你
"""

# describe()
# print(df1.describe())
# print(df1['FONTSIZE'].median())     # 25.0
# print(df1['FONTSIZE'].count())      # 8605
# print(df1['FONTSIZE'].value_counts())
# print(df1['FONTSIZE'].value_counts(ascending=True))  # 设置为True时表示升序，设置为False时表示降序
"""
          PROGRESS            ID         MODE     FONTSIZE
count  8605.000000  8.605000e+03  8605.000000  8605.000000
mean    676.119126  1.400562e+18     1.555375    24.986984
std     425.171698  2.165741e+16     1.382079     0.301581
min       0.000000  0.000000e+00     1.000000    18.000000
25%     259.513000  1.382134e+18     1.000000    25.000000
50%     709.783000  1.413323e+18     1.000000    25.000000
75%    1063.481000  1.413323e+18     1.000000    25.000000
max    1425.900000  1.418357e+18     5.000000    25.000000

FONTSIZE
25    8589
18      16
Name: count, dtype: int64

FONTSIZE
18      16
25    8589
Name: count, dtype: int64
"""

# TODO 4.DataFrame的条件过滤
"""
如同Series一样，我们也可以利用布尔索引来提取DataFrame的子集，从而过滤部分不符合我们要求的数据。
"""
# print(df1[df1.FONTSIZE >= 20])

# 事实上，df[df.salary >=130000]返回的结果是一个与原始DataFrame对象等长度的Series对象，不过匿名了而已，
# 当然我们可以给它赋予一个名称，以备后用。凡是Series对象能使用的属性和方法，这个匿名对象都可以使用。
# 例如，查询FONTSIZE大于20的数量，就可以用“对象名．方法()”的格式，利用count()方法来返回结果。
# print(df1[df1.FONTSIZE >= 20].count())

# 紧抓面向对象编程的精髓，不断通过“.”操作访问DataFrame或Series的方法或属性，便可以接续细化以上结果。
# 比如说，统计查询FONTSIZE大于20并且MODE=5的数量，则可以通过如下代码实现。
# print(df1[df1.FONTSIZE >= 20][df1.MODE == 5]) 会报警
# 推荐写法：（使用 & 组合条件）
# print(df1[(df1.FONTSIZE >= 20) & (df1.MODE == 5)]) # 避免警告：单次索引操作不会触发 reindex

# 从上面返回的结果可以看出，这仍然是一个DataFrame对象。
# 更进一步，如果我们想返回统计查询FONTSIZE大于20并且MODE=5的评论的PROGRESS均值，依然通过一行代码就能“干完收工”。
# print(df1[(df1.FONTSIZE >= 20) & (df1.MODE == 5)].PROGRESS.mean())   # 704.8432638297871

"""
      PROGRESS                CTIME  ...  FONTSIZE            TEXT
0        0.000  2023-08-11 19:43:01  ...        25          梦结束的地方
1        0.000  2023-08-11 20:17:49  ...        25          梦开始的地方
...        ...                  ...  ...       ...             ...
8603  1421.137  2023-09-25 11:39:59  ...        25          爱音的大长腿
8604  1425.900  2023-09-26 19:27:05  ...        25         双排被坑后的你

[8589 rows x 8 columns]

PROGRESS    8589
CTIME       8589
ID          8589
HASH        8589
COLOR       8589
MODE        8589
FONTSIZE    8589
TEXT        8589
dtype: int64

      PROGRESS                CTIME  ...  FONTSIZE                    TEXT
0        0.000  2023-08-11 19:43:01  ...        25                  梦结束的地方
1        0.000  2023-08-11 20:17:49  ...        25                  梦开始的地方
...        ...                  ...  ...       ...                     ...
8601  1418.006  2023-08-12 11:13:39  ...        25                  她要吃人了！
8604  1425.900  2023-09-26 19:27:05  ...        25                 双排被坑后的你

[1175 rows x 8 columns]
"""

# TODO 5.DataFrame的切片操作
"""
DataFrame的切片操作完全模仿NumPy二维数组的切片操作，不过DataFrame中有了行和列的索引，
因此可以通过各种Python语法糖让切片操作更加便捷。比如说，前面我们提到的访问DataFrame的行和列，实际上就是一种切片操作。

当我们想返回5到15行的数据时（作为右边界，取不到编号为15的这一行），就可以通过如下指令完成。

我们还可以通过DataFrame的loc()方法读取特定行和特定列交叉的切片部分。
比如说，我们想读取5到15行的rank、sex和salary这三列的内容，就可以通过如下指令完成切片操作。
"""
# print(df1[5:15])
# print(df1.loc[5:15,['PROGRESS','CTIME']])       # 返回的是第5行至15行  左闭右闭
# print(df1[5:15][['PROGRESS','CTIME']])          # 返回的是第5行至14行  左闭右开

"""
    PROGRESS                CTIME  ...  FONTSIZE               TEXT
5      0.000  2023-09-15 11:52:02  ...        25             梦开始的地方
6      0.000  2023-09-22 19:02:17  ...        25  那时，人们还没意识到事情的严重性（
7      0.000  2023-09-22 22:30:54  ...        25             我，再观看！
8      0.024  2023-08-12 12:49:11  ...        25    注：限时收。看到8月19号为止
9      0.059  2023-09-23 21:30:44  ...        25             我，再观赏！
10     0.104  2023-08-11 22:50:00  ...        25            恭喜你发现宝藏
11     0.167  2023-08-12 00:08:26  ...        25         我的追番键在哪？！！
12     0.174  2023-09-11 14:29:57  ...        25             梦开始的地方
13     0.184  2023-09-17 11:09:01  ...        25             梦开始的地方
14     0.185  2023-08-12 11:16:02  ...        25              我，再观看

[10 rows x 8 columns]

    PROGRESS                CTIME
5      0.000  2023-09-15 11:52:02
6      0.000  2023-09-22 19:02:17
7      0.000  2023-09-22 22:30:54
8      0.024  2023-08-12 12:49:11
9      0.059  2023-09-23 21:30:44
10     0.104  2023-08-11 22:50:00
11     0.167  2023-08-12 00:08:26
12     0.174  2023-09-11 14:29:57
13     0.184  2023-09-17 11:09:01
14     0.185  2023-08-12 11:16:02
15     0.231  2023-08-14 18:33:50

    PROGRESS                CTIME
5      0.000  2023-09-15 11:52:02
6      0.000  2023-09-22 19:02:17
7      0.000  2023-09-22 22:30:54
8      0.024  2023-08-12 12:49:11
9      0.059  2023-09-23 21:30:44
10     0.104  2023-08-11 22:50:00
11     0.167  2023-08-12 00:08:26
12     0.174  2023-09-11 14:29:57
13     0.184  2023-09-17 11:09:01
14     0.185  2023-08-12 11:16:02
"""

# TODO 6.DataFrame的排序操作
"""
在DataFrame中，我们可以根据某一列或某几列，对整个DataFrame中的数据进行排序。默认的排序方式是升序.
"""
df1_sorted = df1.sort_values(by='CTIME')
df1_sorted_decending = df1.sort_values(by='CTIME',ascending=False)
df1_sorted_multicols = df1.sort_values(by=['CTIME','FONTSIZE'],ascending=[True,False])
# print(df1_sorted)
# print(df1_sorted_multicols)

"""
      PROGRESS                CTIME  ...  FONTSIZE                  TEXT
189      4.900  2023-07-13 23:02:44  ...        25                   tyt
178      4.348  2023-08-11 19:00:46  ...        25                    啊？
...        ...                  ...  ...       ...                   ...
420     14.823  2023-10-01 23:00:39  ...        25                 最后一次了
4670   776.231  2023-10-01 23:02:58  ...        25               什么连招哈哈哈

[8605 rows x 8 columns]

      PROGRESS                CTIME  ...  FONTSIZE                  TEXT
189      4.900  2023-07-13 23:02:44  ...        25                   tyt
178      4.348  2023-08-11 19:00:46  ...        25                    啊？
19       0.325  2023-08-11 19:01:11  ...        25                    好耶
913     83.240  2023-08-11 19:01:49  ...        25                  111、
311      9.660  2023-08-11 19:01:50  ...        25                  来历来历
...        ...                  ...  ...       ...                   ...
1776   196.483  2023-10-01 22:59:26  ...        25  这里爱音似乎是没反应过来该用哪个语音回答
3698   598.865  2023-10-01 22:59:40  ...        25                    真姬
3418   544.210  2023-10-01 22:59:43  ...        25               no！！！！！
420     14.823  2023-10-01 23:00:39  ...        25                 最后一次了
4670   776.231  2023-10-01 23:02:58  ...        25               什么连招哈哈哈

[8605 rows x 8 columns]
"""

# TODO 7.Pandas的聚合和分组运算
"""
聚合（Aggregation）和分组其实是紧密相连的。不过在Pandas中，聚合更侧重于描述将多个数据按照某种规则（即特定函数）聚合在一起，
变成一个标量（即单个数值）的数据转换过程。它与张量的“约减”​（reduction）有相通之处。
聚合的流程大致是这样的：先根据一个或多个“键”（通常对应列索引）拆分Pandas对象（Series或DataFrame等）；
然后根据分组信息对每个数据块应用某个函数，这些函数多为统计意义上的函数，
包括但不限于最小值（min）、最大值（min）、平均值（mean）、中位数（median）、
众数（mode）、计数（count）、去重计数（nunique）、求和（min）、标准差（std）、var（方差）、偏度（skew）、峰度（kurt）及用户自定义函数。
我们可以通过Pandas提供的agg()方法来实施聚合操作。
agg()方法仅仅是聚合操作的“壳​，其中的各个参数（即各类操作的函数名）才是实施具体操作的“瓤”。通过设置参数，可以将一个函数作用在一个或多个列上。

例如，如果我们想统计前面提到的数据集合中的薪资（salary）的最小值、最大值、均值及中位数，
利用聚合函数，同时设置多个统计参数，便可“一气呵成”完成工作。
"""
# print(df1.FONTSIZE.agg(['min', 'mean', 'median']))
# print(df1.FONTSIZE.agg(['mode']))

"""
min       18.000000
mean      24.986984
median    25.000000
Name: FONTSIZE, dtype: float64

   mode
0    25
"""

"""
下面讨论一下偏度（Skewness）与峰度（Kurtosis）。或许你会疑问，我们已经有了均值、方差等统计指标了，
为何还需要这两个呢？这两条曲线上的数据拥有相同的均值和方差，你能说这两组数据的分布是一样吗？
它们的分布自然是不同的，但如何评价一组数据的“品相”呢？这时就需要用到偏度和峰度这两个统计指标。
偏度与峰度可用于检测数据集是否符合正态分布。

先来说说偏度。偏度用于衡量随机变量概率分布的不对称性，是相对于平均值不对称程度的度量。
通过对偏度系数进行测量，我们能够判定数据分布的不对称程度及方向。
偏度的衡量是相对于正态分布来说的，正态分布的偏度为0。
若待分析的数据分布是对称的，那么偏度接近于0。
若偏度大于0，则说明数据分布右偏，即分布有一条长尾在右。
若偏度小于0，则数据分布左偏，即分布有一条长尾在左。
很显然，偏度的绝对值越大，分布的偏移程度越严重。

峰度是体现数据分布陡峭或平坦的统计量，通过对峰度系数进行测量，我们能够判定数据相对于正态分布而言是更陡峭还是更平坦。
我们依然将正态分布的峰度作为标杆，其值为0。
若待分析的数据的峰度大于0，则表示该数据分布与正态分布相比较为陡峭，有尖顶峰。
如果峰度小于0，表示该数据总体分布与正态分布相比较为平坦，曲线有平顶峰。
在方差相同的情况下，由于中间部分数据值的方差较小，为了达到和正态分布方差相同的目的，必然会有一些值离中心点较远，即出现异常点（Outlier）。
通常Kurtosis> 3时就存在所谓的“厚尾”（heavy tail）现象，它表明异常点可能增多。
"""

# print(df1[['FONTSIZE', 'PROGRESS']].agg(['skew', 'kurt']))

"""
        FONTSIZE  PROGRESS
skew  -23.130086 -0.091382
kurt  533.124786 -1.335830
"""

# groupby分组
"""
Pandas提供了一个灵活高效的分组方法—groupby()。通过这个方法，我们能以一种很自然的方式，对每个分组进行数据统计、分析和转换。
分组统计的指标包括计数、平均值、标准差，如果标准化的统计不能满足我们的需求，还可以自定义个性化的统计函数。
groupby()的核心操作分三步走：分割-应用-合并（split-apply-combine）。
操作的第一步，就是根据一个或多个key将数据分割为若干个组，每个组包含若干行数据。
分组之后，如果不加以操作，意义并不大，因此我们通常会对分组的结果应用某个特定的函数，产生一组新的值。
然后再将每组产生的值合并到一起，形成一个新的数据集合。
"""
df1_by_fontsize = df1.groupby(['FONTSIZE'])
# print(df1_by_fontsize.mean())
# print(df1_by_fontsize.describe())

# TODO 8.DataFrame的透视表
"""
透视表（pivot table）是一种常见的数据汇总工具。
它能根据一个或多个键对数据进行聚合，并根据行和列上的分组键将数据分配到不同的矩形区域中。
之所以称为透视表，是因为我们可以动态地改变数据的位置分布，以便按照不同方式分析数据，它也支持重新安排行号、列标和字段。
DataFrame对象提供了一个功能强大的pivot_table()方法供我们使用。
此外，Pandas还提供了一个顶级的pandas.pivot_table()函数，二者完成的功能是相同的，其函数原型如下。

pandas.pivot_table(data,values=None,index=None,columns=None,aggfunc='mean',
fill_value=None,margins=False,dropna=True,margins_name='All',observed=False)

data：数据源，就是要分析的DataFrame对象。如果这个函数是以DataFrame对象中的一个方法的身份出现的，
那么这个数据源就是这个DataFrame对象，因此也就没有这个所谓的data参数了。
values：用于聚合操作的列。
index：行层次的分组依据，你可以认为它就是一个分组的键（key），它可以是一个值，
也可以是多个值，如果是多个值，则需要用列表的方括号括起来。
columns：列层次的分组依据，这是一种分割数据的可选方式。在理解上和index类似。
aggfunc：对数据执行聚合操作时所用的函数。当我们未设置aggfunc时，默认aggfunc='mean'，表明计算均值。

默认情况下，DataFrame拥有默认的数字索引（即的最左边一列）。现在我们要构建透视表，就需要提供划分依据，
也就是说，此时pivot_table()需要拥有一个自己独属的index。
比如说，如果我们想按PROGRESS来分组查看,就可以把rank设置为index.
"""
# print(pd.pivot_table(df1, index=['MODE']))

"""
前面我们是用Pandas的全局函数pivot_table()来生成透视表的，DataFrame的对象df是作为这个函数的参数传递进去的。
而实际上，对象df本身就有成员方法pivot_table()，我们可以用点（.）操作完成这个方法的调用，从而达到相同的透视效果下。
"""
# print(df1.pivot_table(index=['MODE']))

"""
有时候，为了更好地进行观察，我们可能需要将行列实施转置。
这时，就需要用到DataFrame对象的另一个方法unstack().
"""
# df1.pivot_table(index=['MODE']).unstack()

# nstack()的反操作就是stack()，它的功能是把一个列索引转置为行索引，
# print(df1.pivot_table(index=['MODE']).unstack().stack())

"""
对于unstack()和stack()方法的使用，需要注意如下几点。
unstack()：将数据的行“旋转”为列。
stack()：将数据的列“旋转”为行。
如果不指定旋转的索引级别，stack()和unstack()默认对最内层进行操作（即level=-1，这里的“-1”表示倒数第一层）。
stack()和unstack()为一组逆运算操作。
"""

"""
在pivot_table()方法中还有第二个好用的参数—values，它就是用来指特定字段的，从而达到筛选所需透视列的目的。
"""
# print(df1.pivot_table(index=['MODE','PROGRESS'],values='CTIME'))  # 仅对CTIMEy这一列实施MODE和PROGRESS级别的透视。

"""
前面我们完成的所有透视操作，其实都是对数值型的列“均值”（mean）进行的透视操作，这是该方法的默认设置。
假设某个财务部门除了想获取某些列的均值，还想透视一下该列的整体输入（sum），该怎么办呢？
这时，就需要启用pivot_table()的第三个重要参数aggfunc。
在这个参数里，我们可以设定一个或多个聚合函数。

和前面讨论聚合函数时的描述类似，如果aggfunc中的函数来自第三方（如来自NumPy或自定义），则需直接给出函数名，但无须给出函数后跟随的一对括号。
如果函数是Pandas内置的函数，如sum、std、var等，则需要给出这些函数名的字符串（用引号将函数名引起来）。
如果聚合操作有多个，则需要将这些实施聚合操作的函数名打包成一个列表，一并给出。
"""
# print(df1.pivot_table(index=['MODE','PROGRESS'],values='CTIME',aggfunc=[np.mean,'sum']))

"""
学习到这里，或许你会发现，透视表实现的功能好像在哪里见过，有种“似曾相识燕归来”的感觉。是的！
它就是我们前面提到的groupby()方法。二者在很多方面的功能都是等价的。通常来说，下面两种调用方式在功能上是等价的。
"""
# df1.pivot_table(index=[key1,key2],values=[kay3,kay4],aggfunc=[函数1,函数2])
# df1.groupby([key1,key2])[key3,key4].agg([函数1,函数2])

# TODO 9.DataFrame的类SQL操作
# print(df1[df1['MODE'] == 5].head(10))
# print(df1.query("MODE == 5").head(10))

"""
    PROGRESS                CTIME  ...  FONTSIZE              TEXT
0      0.000  2023-08-11 19:43:01  ...        25            梦结束的地方
1      0.000  2023-08-11 20:17:49  ...        25            梦开始的地方
2      0.000  2023-08-12 17:24:16  ...        25           我 邦 要 火
5      0.000  2023-09-15 11:52:02  ...        25            梦开始的地方
8      0.024  2023-08-12 12:49:11  ...        25   注：限时收。看到8月19号为止
10     0.104  2023-08-11 22:50:00  ...        25           恭喜你发现宝藏
21     0.424  2023-08-12 12:01:46  ...        25           梦想开始的地方
23     0.468  2023-09-23 01:30:08  ...        25           进度条是雨伞！
28     0.600  2023-09-15 17:21:11  ...        25  换名字了！！！官方换名字了！！！
29     0.614  2023-09-06 13:15:58  ...        25          GO学家，启动！

[10 rows x 8 columns]
"""

"""
从输出效果来看，df.query("rank=='Prof'")完全等价于df[df['rank']=='Prof']。但显然前者更加简明扼要，可读性也更好。
这里需要注意的是，使用query()方法时，查询字符串需要用双引号引起来，如果查询字符串里还有索引名称（如Prof），还需要再次用单引号引起来。
"""

"""
为了提高查询的灵活性，我们还可以用变量指明查询条件。
在Pandas中，需要用特定的格式表明变量身份，这里variable_name就是变量名，任意符合Python命名规则的变量均可使用。
在查询字符串中，变量名前面的符号“@”不可缺少。
"""
MODE1 = 5
# print(df1.query("MODE == @MODE1").head(5))
"""
   PROGRESS                CTIME  ...  FONTSIZE             TEXT
0     0.000  2023-08-11 19:43:01  ...        25           梦结束的地方
1     0.000  2023-08-11 20:17:49  ...        25           梦开始的地方
2     0.000  2023-08-12 17:24:16  ...        25          我 邦 要 火
5     0.000  2023-09-15 11:52:02  ...        25           梦开始的地方
8     0.024  2023-08-12 12:49:11  ...        25  注：限时收。看到8月19号为止

[5 rows x 8 columns]
"""

# TODO 10.DataFrame中的数据清洗方法
"""
缺失值检测：isnull()/notnull()
缺失值填充：fillna(0)                     默认填充0
缺失值丢弃：dropna()                      丢弃
        dropna(how='all')               当前单元格所在的行或列都为缺失值（NaN）时，则抛弃数据
        dropna(axis = 1,how='all')      当列方向的所有数据都为缺失值时，抛弃该列
        dropna(axis = 1,how='any')      当列方向的数据有任何一个缺失值时，抛弃该列
        dropna(thresh = 5)              当所在行的数据有效值低于5个时，则抛弃该行，这里的thresh是可修改得阈值
         
"""